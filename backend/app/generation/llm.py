# app/generation/llm.py

def generate_answer(prompt: str) -> str:
    """
    Generate an answer from a prompt.
    This is a stub implementation (no real LLM call yet).
    """
    return (
        "⚠️ Answer generation is not implemented yet.\n\n"
        "Prompt received:\n"
        "----------------\n"
        f"{prompt}"
    )
